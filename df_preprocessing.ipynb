{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy\n",
    "import openml\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Profilling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **An Exploratory Data Analysis of U.S Individual Donations for Political Campaigs**\n",
    "\n",
    "\n",
    "---\n",
    "A Comissão Federal de Eleições (FEC) é uma agência independente que visa mediar e regular o financiamente de campanhas presidencias dos EUA. Sob o pretexto de registrar cada doação dos cidadãos, criou-se a base que será aqui trabalhada. A partir das bibliotecas **Vega-Altair** e **Plotly**, explora-se a criação de visualizações para compreender e investigar o comportamento das contribuições políticas ao redor do país.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **As variáveis da base**\n",
    "\n",
    "#### Variáveis Preditivas\n",
    "\n",
    "* **cmte_id**: ID do comite para o qual a contribuição foi feita. Cada comitê possui um identificador único. Neste dataset estão presentes 7112 comites diferentes.\n",
    "\n",
    "* **amndt_ind**: Indicador de Emenda. Tal variável é usada para mostrar se uma transação ou um registro é uma emenda a um registro anteriormente apresentado, sendo classificada por uma das três opções:\n",
    "  * N - Novo registro;\n",
    "\n",
    "  * A - Emenda a um registro anterior;\n",
    "  * C - Correção/alteração num registro.\n",
    "\n",
    "* **rpt_tp**: Tipo de Relatório de contribuição do Comitê ao qual o contribuinte está associado. Seus tipos são comumente diferenciados pela sua recorrência(mensal, trimestral, pré-eleição e etc).\n",
    "\n",
    "* **transaction_pgi**: Indicador referente a qual etapa do ciclo eleitoral a contribuição em questão está associada.\n",
    "  *  G - General Election, fase final do ciclo, onde um candidato é eleito para o cargo\n",
    "  *  P - Primary Election, refere-se às eleições que são internas aos partidos\n",
    "  *  S - Special Election, eleições convocadas fora do ciclo eleitoral regular para preencher vagas inesperadas\n",
    "  *  E - Election,\n",
    "  *  C - Convenction, Convenções Partidárias, que podem ser usadas para selecionar candidatos ou tomar outras decisões internas do partido.\n",
    "  * O - Other,\n",
    "\n",
    "  * R - Runoff Election (Segundo Turno)\n",
    "\n",
    "  * 0 - ??\n",
    "\n",
    "* **image_num**: Identificador único associado à imagem digitalizada do relatório financeiro.\n",
    "\n",
    "* **transaction_tp**\n",
    "  * 10 - Contribuição Financeira direta de um indivíduo ou entidade para um comitê de campanha de um candidato\n",
    "  * 11 - Contribuição de uma Tribo Nativa\n",
    "  * 15 - Contribuição financeira direta feita por um indivíduo, sociedade ou LLC (Empresa de responsabilidade limitada) para comitês políticos tradicionais, excluindo Super PACs e Hybrid PACs.\n",
    "  * 15C - Contribuição de candidatos\n",
    "  * 15E - Contribuições Direcionadas feitas por indivíduos, sociedades ou LLC para comitês políticos tradicionais\n",
    "  * 19 - Doação para comunicação eleitoral\n",
    "\n",
    "  * 20Y - Reembolso de Fundos não Eleitorais. Devolução de fundos que foram originalmente atribuídos a atividades não eleitorais.\n",
    "  * 21Y - Reembolso de uma Tribo Nativa\n",
    "  * 22Y - Reembolso de uma contribuição de uma indivíduo, parceiros ou uma empresa de responsabilidade limitada\n",
    "  * 24I - Contribuição Direcionada por cheque\n",
    "  * 24T - Contribuição Direcionada utilizando fundos do tesouro do comitê intermediário\n",
    "\n",
    "* **entity_tp**\n",
    "  * IND - Individual\n",
    "  * ORG - Organização\n",
    "  * CCM - Comitê do Candidato\n",
    "  * PAC - Comitê de Ação Política\n",
    "  * CAN - Candidato\n",
    "  * COM - Comitê\n",
    "\n",
    "  * PTY - Comitê Oficial do Partido\n",
    "\n",
    "* **name** - Nome do Contribuinte\n",
    "* **city** - Cidade do Contribuinte\n",
    "\n",
    "* **state** - Estado do Contribuinte\n",
    "\n",
    "* **zip_code** - Código Postal do Contribuinte\n",
    "\n",
    "* **employer** - Empregador do Contribuinte\n",
    "\n",
    "* **ocuppation** - Emprego do Contribuinte\n",
    "\n",
    "* **transaction_dt** - Data de transação das contribuições\n",
    "\n",
    "* **other_id** - ID de identificação de uma terceira parte envolvida na transação\n",
    "\n",
    "* **tran_id** - ID único da transação\n",
    "\n",
    "* **file_num** - Número do arquivo\n",
    "\n",
    "* **memo_cd** - Identificador associado a presença de um memorando na transação, o qual fornece um contexto adicional à transação\n",
    "\n",
    "* **memo_text** - Texto do memorando\n",
    "\n",
    "*  **sub_id** - Identificador de submissão\n",
    "\n",
    "#### Variável Alvo\n",
    "* **transaction_amt**: O valor da transação em dólares. Indica a quantidade de dinheiro envolvida em uma contrbuição individual para campanhas políticas.\n",
    "\n",
    "Assim, o dataset é composto por 5 variáveis de Identificação única, 13 variáveis categóricas, uma variável de texto, uma variável de data e uma varável numérica (alvo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(42080)\n",
    "df, _, _, _ = dataset.get_data(dataset_format='dataframe')\n",
    "path_shapefile = \"cb_2018_us_state_500k\\cb_2018_us_state_500k.shp\"\n",
    "\n",
    "state_boundaries = gpd.read_file(path_shapefile, columns=['STUSPS', 'NAME', 'geometry'])\n",
    "state_boundaries.rename(columns={'STUSPS': 'state',\n",
    "                           'Name': 'state_name'}, inplace=True)\n",
    "\n",
    "df_EUA = state_boundaries.merge(df, on='state', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3348209, 21)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tran_id            2999272\n",
      "name               1534998\n",
      "employer            657449\n",
      "occupation          145304\n",
      "zip_code             86334\n",
      "city                 27709\n",
      "cmte_id               7112\n",
      "other_id              2543\n",
      "state                   67\n",
      "rpt_tp                  26\n",
      "transaction_tp          11\n",
      "transaction_pgi          8\n",
      "entity_tp                7\n",
      "amndt_ind                3\n",
      "memo_cd                  2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Número de categorias únicas das variáveis categóricas\n",
    "categorical_cols = df.select_dtypes(include='object')\n",
    "num_categories = categorical_cols.nunique().sort_values(ascending=False)\n",
    "print(num_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memo_cd            97.314325\n",
      "other_id           92.992313\n",
      "memo_text          87.438747\n",
      "transaction_pgi    28.104966\n",
      "employer           10.371216\n",
      "occupation          5.546966\n",
      "state               0.258556\n",
      "city                0.064990\n",
      "entity_tp           0.042948\n",
      "transaction_dt      0.010573\n",
      "tran_id             0.009766\n",
      "name                0.004211\n",
      "file_num            0.000030\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Percentual de Dados Faltantes\n",
    "missing_percent = df.isna().sum().sort_values(ascending=False) / df.shape[0] * 100\n",
    "missing_percent = missing_percent[missing_percent > 0]\n",
    "print(missing_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occupation</th>\n",
       "      <th>employer</th>\n",
       "      <th>transaction_pgi</th>\n",
       "      <th>memo_cd</th>\n",
       "      <th>memo_text</th>\n",
       "      <th>other_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1890857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>683124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>93993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>88439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>47905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    occupation  employer  transaction_pgi  memo_cd  memo_text  other_id  \\\n",
       "7            0         0                0        1          1         1   \n",
       "15           0         0                1        1          1         1   \n",
       "54           1         1                0        1          1         1   \n",
       "5            0         0                0        1          0         1   \n",
       "23           0         1                0        1          1         1   \n",
       "4            0         0                0        1          0         0   \n",
       "12           0         0                1        1          0         0   \n",
       "13           0         0                1        1          0         1   \n",
       "31           0         1                1        1          1         1   \n",
       "1            0         0                0        0          0         1   \n",
       "62           1         1                1        1          1         1   \n",
       "6            0         0                0        1          1         0   \n",
       "20           0         1                0        1          0         0   \n",
       "8            0         0                1        0          0         0   \n",
       "14           0         0                1        1          1         0   \n",
       "\n",
       "      count  \n",
       "7   1890857  \n",
       "15   683124  \n",
       "54   140372  \n",
       "5     93993  \n",
       "23    88439  \n",
       "4     85737  \n",
       "12    80218  \n",
       "13    60084  \n",
       "31    52315  \n",
       "1     47905  \n",
       "62    23054  \n",
       "6     12113  \n",
       "20    10334  \n",
       "8     10022  \n",
       "14     9442  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Co-ocorrência das variáveis com valores faltantes significantes\n",
    "significant_missing_columns = ['occupation', 'employer', 'transaction_pgi', 'memo_cd', 'memo_text', 'other_id']\n",
    "co_occur = df[significant_missing_columns].isna().astype(int)\n",
    "co_occur_agg = co_occur.groupby(significant_missing_columns).size().reset_index(name='count').sort_values(by='count', ascending=False)\n",
    "co_occur_agg.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occupation</th>\n",
       "      <th>employer</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2994621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>179386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>167864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   occupation  employer    count\n",
       "0           0         0  2994621\n",
       "3           1         1   179386\n",
       "1           0         1   167864\n",
       "2           1         0     6338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Co-ocorrência das variáveis com dados faltantes que podem ser imputadas\n",
    "significant_missing_columns = ['occupation', 'employer']\n",
    "co_occur = df[significant_missing_columns].isna().astype(int)\n",
    "co_occur_agg = co_occur.groupby(significant_missing_columns).size().reset_index(name='count').sort_values(by='count', ascending=False)\n",
    "co_occur_agg.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclui-se as variáveis de indentificadores únicos\n",
    "df.drop(columns=['name', 'sub_id', 'tran_id', 'image_num'], inplace=True)\n",
    "\n",
    "# Exclui-se as variáveis 'memo_cd', 'other_id', 'memo_text'\n",
    "df.drop(columns=['memo_cd', 'other_id', 'memo_text'], inplace=True)\n",
    "\n",
    "colunas_com_poucos_dados_faltantes = [\"state\", \"city\", \"entity_tp\", \"transaction_dt\", \"file_num\"]\n",
    "df = df.dropna(subset=colunas_com_poucos_dados_faltantes)\n",
    "df['transaction_pgi'] = df['transaction_pgi'].fillna(\"Desconhecido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transaction_dt'] = df['transaction_dt'].astype(int).astype(str)\n",
    "\n",
    "# Garantir que todos os valores tenham 8 dígitos (ex.: '9122012' -> '09122012')\n",
    "df['transaction_dt'] = df['transaction_dt'].str.zfill(8)\n",
    "\n",
    "# Criar uma coluna datetime usando pd.to_datetime\n",
    "df['transaction_dt'] = pd.to_datetime(df['transaction_dt'], format='%m%d%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data imputation with word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fraction = 0.01  # 1% dos dados\n",
    "np.random.seed(42)  # Para reprodutibilidade\n",
    "\n",
    "# Selecionar uma amostra de 1% dos dados\n",
    "sampled_df = df.sample(frac=sample_fraction, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Handle missing values\n",
    "    if pd.isnull(text):\n",
    "        return 'missing'\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Remove common suffixes and stopwords\n",
    "    text = re.sub(r'\\b(inc|llc|corp|corporation|co|ltd|and|the)\\b', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "for col in ['employer', 'occupation']:\n",
    "    sampled_df[col] = sampled_df[col].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for employers from the sample...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Employers Progress: 100%|██████████| 15343/15343 [03:02<00:00, 84.00employer/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for occupations from the sample...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Occupations Progress: 100%|██████████| 5165/5165 [00:58<00:00, 88.16occupation/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputing missing values...\n"
     ]
    }
   ],
   "source": [
    "# Garantir que a coluna employer e occupation tenha valores únicos na amostra\n",
    "unique_employers = sampled_df[sampled_df['employer'] != 'missing']['employer'].unique()\n",
    "unique_occupations = sampled_df[sampled_df['occupation'] != 'missing']['occupation'].unique()\n",
    "\n",
    "# Carregar o modelo\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Gerar embeddings para employers\n",
    "employer_embeddings = {}\n",
    "print(\"Generating embeddings for employers from the sample...\")\n",
    "for employer in tqdm(unique_employers, desc=\"Employers Progress\", unit=\"employer\"):\n",
    "    embedding = model.encode(employer)\n",
    "    employer_embeddings[employer] = embedding\n",
    "\n",
    "# Gerar embeddings para occupations\n",
    "occupation_embeddings = {}\n",
    "print(\"\\nGenerating embeddings for occupations from the sample...\")\n",
    "for occupation in tqdm(unique_occupations, desc=\"Occupations Progress\", unit=\"occupation\"):\n",
    "    embedding = model.encode(occupation)\n",
    "    occupation_embeddings[occupation] = embedding\n",
    "\n",
    "# Função para imputar os valores ausentes\n",
    "def impute_missing_values(row, embeddings_dict, column_name, model, fallback_value):\n",
    "    if row[column_name] == 'missing':\n",
    "        other_column = 'occupation' if column_name == 'employer' else 'employer'\n",
    "        context = row[other_column]\n",
    "        if context != 'missing':\n",
    "            context_embedding = model.encode(context)\n",
    "            similarities = cosine_similarity([context_embedding], list(embeddings_dict.values()))[0]\n",
    "            max_similarity = np.max(similarities)\n",
    "            if max_similarity >= 0.7:\n",
    "                most_similar_index = np.argmax(similarities)\n",
    "                return list(embeddings_dict.keys())[most_similar_index]\n",
    "    return fallback_value\n",
    "\n",
    "# Imputar os valores ausentes na amostra\n",
    "print(\"\\nImputing missing values...\")\n",
    "sampled_df['employer'] = sampled_df.apply(\n",
    "    lambda row: impute_missing_values(\n",
    "        row, employer_embeddings, 'employer', model, sampled_df['employer'].mode()[0]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "sampled_df['occupation'] = sampled_df.apply(\n",
    "    lambda row: impute_missing_values(\n",
    "        row, occupation_embeddings, 'occupation', model, sampled_df['occupation'].mode()[0]\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Validar distribuições após a imputação\n",
    "print(sampled_df['employer'].isna().sum())\n",
    "print(sampled_df['occupation'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.to_csv(\"amostra.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoding from zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"zip_code_locator\")\n",
    "\n",
    "# Cache para armazenar os resultados de ZIP codes já consultados\n",
    "cache = {}\n",
    "\n",
    "def get_location_from_zip(zip_code, max_retries=5):\n",
    "    if zip_code in cache:\n",
    "        print(f\"zip code: {zip_code} já processado.\")\n",
    "        return zip_code, *cache[zip_code]\n",
    "    \n",
    "    # Implementar retentativas\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            location = geolocator.geocode({\"postalcode\": zip_code, \"country\": \"USA\"})\n",
    "            if location:\n",
    "                result = (location.latitude, location.longitude, location.address.split(',')[1].strip())\n",
    "                cache[zip_code] = result\n",
    "                time.sleep(1.5)  # Reduz carga no serviço gratuito\n",
    "                return zip_code, *result\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na tentativa {attempt + 1} para o ZIP code {zip_code}: {e}\")\n",
    "            time.sleep(2 ** attempt)  # Espera progressiva (backoff exponencial)\n",
    "    \n",
    "    # Retornar None se todas as tentativas falharem\n",
    "    return zip_code, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 já processado. Pulando...\n",
      "Chunk 2 já processado. Pulando...\n",
      "Chunk 3 já processado. Pulando...\n",
      "Chunk 4 já processado. Pulando...\n",
      "Chunk 5 já processado. Pulando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 6: 100%|██████████| 2000/2000 [1:23:49<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 6 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 7: 100%|██████████| 2000/2000 [1:23:32<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 7 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 8: 100%|██████████| 2000/2000 [1:23:45<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 8 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 9: 100%|██████████| 2000/2000 [1:15:32<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 9 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 10: 100%|██████████| 2000/2000 [1:01:17<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 10 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 11: 100%|██████████| 2000/2000 [1:11:55<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 11 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 12: 100%|██████████| 2000/2000 [1:23:45<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 12 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 13: 100%|██████████| 2000/2000 [1:23:53<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 13 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 14: 100%|██████████| 2000/2000 [1:23:58<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 14 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 15: 100%|██████████| 2000/2000 [12:14:53<00:00, 22.05s/it]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 15 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 16: 100%|██████████| 2000/2000 [1:24:53<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 16 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 17: 100%|██████████| 2000/2000 [1:23:51<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 17 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 18:  38%|███▊      | 767/2000 [32:15<51:18,  2.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro na tentativa 1 para o ZIP code 95290.0: Non-successful status code 500\n",
      "Erro na tentativa 2 para o ZIP code 95290.0: Non-successful status code 500\n",
      "Erro na tentativa 3 para o ZIP code 95290.0: Non-successful status code 500\n",
      "Erro na tentativa 4 para o ZIP code 95290.0: Non-successful status code 500\n",
      "Erro na tentativa 5 para o ZIP code 95290.0: Non-successful status code 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 18:  38%|███▊      | 768/2000 [32:48<3:56:01, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro na tentativa 1 para o ZIP code 32694.0: Non-successful status code 500\n",
      "Erro na tentativa 2 para o ZIP code 32694.0: Non-successful status code 500\n",
      "Erro na tentativa 3 para o ZIP code 32694.0: Non-successful status code 500\n",
      "Erro na tentativa 4 para o ZIP code 32694.0: Non-successful status code 500\n",
      "Erro na tentativa 5 para o ZIP code 32694.0: Non-successful status code 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 18:  38%|███▊      | 769/2000 [33:20<6:02:59, 17.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro na tentativa 1 para o ZIP code 21661.0: Non-successful status code 500\n",
      "Erro na tentativa 2 para o ZIP code 21661.0: Non-successful status code 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 18: 100%|██████████| 2000/2000 [1:25:03<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 18 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 19: 100%|██████████| 2000/2000 [1:23:33<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 19 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 20: 100%|██████████| 2000/2000 [1:23:40<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 20 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 21: 100%|██████████| 2000/2000 [2:41:02<00:00,  4.83s/it]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 21 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 22: 100%|██████████| 2000/2000 [1:23:35<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 22 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 23: 100%|██████████| 2000/2000 [1:23:36<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 23 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_23.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 24: 100%|██████████| 2000/2000 [1:17:35<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 24 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_24.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 25: 100%|██████████| 2000/2000 [1:03:12<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 25 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_25.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 26: 100%|██████████| 2000/2000 [1:19:16<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 26 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_26.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 27: 100%|██████████| 2000/2000 [1:09:42<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 27 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_27.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 28: 100%|██████████| 2000/2000 [1:05:26<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 28 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_28.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 29: 100%|██████████| 2000/2000 [1:22:25<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 29 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_29.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 30: 100%|██████████| 2000/2000 [1:11:35<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 30 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_30.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 31: 100%|██████████| 2000/2000 [1:18:29<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 31 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 32: 100%|██████████| 2000/2000 [1:17:29<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 32 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_32.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 33: 100%|██████████| 2000/2000 [1:23:08<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 33 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_33.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 34: 100%|██████████| 2000/2000 [1:23:45<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 34 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_34.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 35: 100%|██████████| 2000/2000 [1:23:34<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 35 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_35.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 36: 100%|██████████| 2000/2000 [1:23:37<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 36 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_36.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 37: 100%|██████████| 2000/2000 [1:23:30<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 37 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_37.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 38: 100%|██████████| 2000/2000 [1:10:56<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 38 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_38.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 39: 100%|██████████| 2000/2000 [1:24:12<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 39 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_39.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 40: 100%|██████████| 2000/2000 [1:08:15<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 40 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_40.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 41: 100%|██████████| 2000/2000 [1:23:45<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 41 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_41.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 42: 100%|██████████| 2000/2000 [1:21:37<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 42 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_42.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando chunk 43: 100%|██████████| 1802/1802 [1:15:07<00:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 43 salvo com sucesso em geoprocessed_chunks\\processed_zip_codes_chunk_43.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unique_zip_codes = df['zip_code'].unique()\n",
    "\n",
    "chunk_size = 2000\n",
    "output_dir = \"geoprocessed_chunks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Verificar quais chunks já foram processados\n",
    "processed_chunks = {int(file.split('_')[-1].split('.')[0]) for file in os.listdir(output_dir) if file.endswith(\".csv\")}\n",
    "\n",
    "for i in range(0, len(unique_zip_codes), chunk_size):\n",
    "    chunk_index = i // chunk_size + 1  \n",
    "\n",
    "    if chunk_index in processed_chunks:\n",
    "        print(f\"Chunk {chunk_index} já processado. Pulando...\")\n",
    "        continue\n",
    "\n",
    "    chunk = unique_zip_codes[i:i+chunk_size]\n",
    "    \n",
    "    results = []\n",
    "    for zip_code in tqdm(chunk, desc=f\"Processando chunk {chunk_index}\"):\n",
    "        results.append(get_location_from_zip(zip_code))\n",
    "    \n",
    "    chunk_df = pd.DataFrame(results, columns=['zip_code', 'latitude', 'longitude', 'cidade'])\n",
    "    \n",
    "    chunk_file_name = os.path.join(output_dir, f\"processed_zip_codes_chunk_{chunk_index}.csv\")\n",
    "    chunk_df.to_csv(chunk_file_name, index=False)\n",
    "    print(f\"Chunk {chunk_index} salvo com sucesso em {chunk_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85802, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"geoprocessed_chunks\"\n",
    "\n",
    "all_chunk_files = glob.glob(os.path.join(output_dir, \"processed_zip_codes_chunk_*.csv\"))\n",
    "processed_chunks_df = pd.concat([pd.read_csv(file) for file in all_chunk_files], ignore_index=True)\n",
    "processed_chunks_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>cidade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28791.0</td>\n",
       "      <td>35.348586</td>\n",
       "      <td>-82.502214</td>\n",
       "      <td>Henderson County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39475.0</td>\n",
       "      <td>31.162820</td>\n",
       "      <td>-89.414966</td>\n",
       "      <td>Lamar County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61834.0</td>\n",
       "      <td>40.158154</td>\n",
       "      <td>-87.627952</td>\n",
       "      <td>Danville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75462.0</td>\n",
       "      <td>33.668733</td>\n",
       "      <td>-95.493766</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24740.0</td>\n",
       "      <td>37.369439</td>\n",
       "      <td>-81.096546</td>\n",
       "      <td>Princeton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85797</th>\n",
       "      <td>7505.0</td>\n",
       "      <td>40.916434</td>\n",
       "      <td>-74.171520</td>\n",
       "      <td>Paterson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85798</th>\n",
       "      <td>7843.0</td>\n",
       "      <td>40.938197</td>\n",
       "      <td>-74.660831</td>\n",
       "      <td>Hopatcong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85799</th>\n",
       "      <td>10925.0</td>\n",
       "      <td>41.211813</td>\n",
       "      <td>-74.301091</td>\n",
       "      <td>Village of Greenwood Lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85800</th>\n",
       "      <td>7035.0</td>\n",
       "      <td>40.918130</td>\n",
       "      <td>-74.309320</td>\n",
       "      <td>Lincoln Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85801</th>\n",
       "      <td>7020.0</td>\n",
       "      <td>40.821053</td>\n",
       "      <td>-73.980033</td>\n",
       "      <td>Edgewater</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85802 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      zip_code   latitude  longitude                     cidade\n",
       "0      28791.0  35.348586 -82.502214           Henderson County\n",
       "1      39475.0  31.162820 -89.414966               Lamar County\n",
       "2      61834.0  40.158154 -87.627952                   Danville\n",
       "3      75462.0  33.668733 -95.493766                      Paris\n",
       "4      24740.0  37.369439 -81.096546                  Princeton\n",
       "...        ...        ...        ...                        ...\n",
       "85797   7505.0  40.916434 -74.171520                   Paterson\n",
       "85798   7843.0  40.938197 -74.660831                  Hopatcong\n",
       "85799  10925.0  41.211813 -74.301091  Village of Greenwood Lake\n",
       "85800   7035.0  40.918130 -74.309320               Lincoln Park\n",
       "85801   7020.0  40.821053 -73.980033                  Edgewater\n",
       "\n",
       "[85802 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_chunks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121979, 16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_1 = processed_chunks_df[['zip_code', 'latitude', 'longitude']]\n",
    "amostra_teste = pd.merge(merge_1, df, on='zip_code', how='inner')\n",
    "amostra_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zip_code               0\n",
       "latitude           12326\n",
       "longitude          12326\n",
       "cmte_id                0\n",
       "amndt_ind              0\n",
       "rpt_tp                 0\n",
       "transaction_pgi        0\n",
       "transaction_tp         0\n",
       "entity_tp              0\n",
       "city                   0\n",
       "state                  0\n",
       "employer           12498\n",
       "occupation          6905\n",
       "transaction_dt         0\n",
       "transaction_amt        0\n",
       "file_num               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amostra_teste.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amostra_teste.to_csv(\"amostra_coordenadas.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
